---
title: "2025 年终杂谈：关于Agent的思考"
date: 2025-12-31
abstract: "围绕 Agent Learning/Evolving、AI 产品的方法论、行业竞争与长期壁垒，以及人如何在不确定中重构“奖励”，展开的一篇年终杂谈。"
tags: ["Agent"]
author: "Jinyu Xiang"
featured: false
draft: true
---

# 引言

投身创业已有半年的时间，深度参与产品迭代的同时，也一直在Research的一线关注大家的工作，带团队的同学做一些探索，这个过程中记录了许多零散的思考，年终了把它们做了一些整理，不一定都对，但先记下来再说。

> **TL;DR**
> - **Agent 在"学什么"比"怎么学"更重要**：很多工作在卷 Pipeline，但更关键的是提炼能跨任务泛化的元能力，而不是对规则的过拟合。
> - **Human 不是评审者，而是环境的一部分**：Learning from Human 可以视为 Learning from a Dynamic Environment 的特例，产品交互本身在塑造世界。
> - **"学习"与"执行"递归同构**：优化者观测执行者轨迹、输出参数更新，遵循相同的 ReAct 范式——学任务和做任务本质同构。
> - **Budget 是产品变量**：从 1 到 100+ 的步数/成本/延迟，决定了 Agent 的行为模式；同一需求在不同预算下应该有不同解法。
> - **长期壁垒来自元能力与迭代回路**：当模型趋于公共基础设施，竞争关键在"制造能力的能力"（自举、速度、创新管线）。

## 一、关于 Agent 的思考

### AI 到底在学什么？

当我们谈论 Agent Learning 时，大家往往沉迷于构建各种复杂的 Pipeline，设计精妙的反馈机制，试图从 Agent 之前的轨迹中榨取价值。这是 "How" 的层面，也是目前 Research 最卷的领域。

但很少有人停下来思考：**Agent 到底在学什么？**

它是在死记硬背某种特定环境下的规则（Overfitting to rules），还是真的从这些轨迹中提取出了能够跨任务泛化的**元能力（Meta-Capability）**？

过去我们常把 **Learning from Env** 和 **Learning from Human** 分开讨论。但这种二分法可能是一种人为的割裂。

当我们将产品定义为基础设施（Infra）时，用户的每一次点击、每一条反馈、每一个偏好信号，都在实时地塑造着 Agent 所处的"世界"。从这个意义上说，**用户本质上就是环境（Environment）的一部分**，他们是系统中最活跃、最具不确定性的环境变量。

因此，Learning from Human 其实是 Learning from a Dynamic Environment 的特例——只不过这个环境变量恰好是人类。

更进一步的问题是：**为什么要学这些东西？** 我们构建数据集、设计奖励函数，本质上是在定义 Agent 的价值观和能力边界。如果 "What"（学习目标）本身是偏差的，那么再完美的 "How"（学习方法）也只能让 Agent 在错误的道路上越跑越远。

### Less is more

在 Agent 和算法的设计中，我看到许多工作在堆砌复杂度——设计错综复杂的 Multi-Agent 拓扑，构建冗长的 Workflow，仿佛结构越复杂，性能就越强。

但我的观点始终未变：**一个真正健壮的复杂系统，往往是从极简的结构中"生长"出来的（Emergence），而不是通过预设的复杂性堆出来的。**

当然，这并非绝对的教条——操作系统和数据库等成熟的工程系统往往是精心设计的产物。但对于当前阶段的 Agent 系统而言，我们对任务空间和交互模式的理解还远未成熟，过早地硬编码复杂结构反而会限制系统的适应性。

因此，我们不应试图在初始阶段就以上帝视角规定所有的交互路径。预先假定的复杂结构往往带来的是脆弱性，而非性能的线性提升。在探索阶段，简单的规则演化出复杂的行为，这才是 Agent 系统设计的务实之道。

### Agent 的 Budget 设计

我们可以把 Chatbot 理解为 Budget 为 1 step 的特殊 Agent：它只有一次回复的机会，且动作空间仅限于文本生成。

而一个通用的 Agent，其 Max Steps（Budget）可能是 100 steps 甚至更多。Budget的不同，直接决定了任务执行的深度与广度（区别是质量，时间，成本）。

我们太强调一个 Agent“能做无限步”，但忽视了从产品设计的角度来看，在1到无限之间，还有中间那么多的Budget状态，同样的 Query（例如“我要一份市场调研报告”），用户背后的隐性需求可能是“我很急，给我个概览”（Low Budget），也可能是“我不急，我要最深度的挖掘”（High Budget）。Agent 产品需要能够理解这种隐性的资源约束，Agent则在不同的 Budget 下都能给出不同的行为模式。

### Agent的有限与离散

用物理学的视角来看，当前的 Agent 面临着两个本质的约束：**有限的空间（Context）**与**离散的时间（Discrete Steps）**。

1.  **记忆的有限性（Space Limit）**：无论 Context Window 如何扩大，Agent 依然缺乏真正的长期记忆机制。这导致它目前主要适用于完成一次性的、无状态的任务。在面对需要长期上下文积累或复杂状态维护的场景时，即便是最先进的模型也会显得力不从心。
2.  **感知的离散性（Time Discreteness）**：Agent 的感知不是连续的流，而是一组离散的 Tokens 或 Screenshot。这在 GUI 操作或实时交互中暴露得尤为明显。如果页面动态更新的速度超过了 Agent 的“帧率”（时间步长），它就会丢失关键信息，甚至产生幻觉。

### 跨越时空的 Agent

在设计 Agent 产品时，有两个重要的点值得关注：

1.  **跨越空间**：让 Agent 在不同的领域、不同的数据库、不同的软件生态之间自由流转，打通信息孤岛。这是目前大多数 Agent 产品的发力点（Tool Use & Integration）。
2.  **跨越时间**：让 Agent 具备长时序的因果推理能力。例如，Agent 今天的某个决策，在一个月后产生了什么结果？Agent 能否根据这个延迟反馈（Delayed Feedback）来修正自己一个月前的策略？

目前的 Agent 大多只聚焦在在“跨越空间”的部分，忙于对接各种 API。但“跨越时间”的Agent，能在漫长的时间跨度上追踪因果，进行长周期的优化。

### 理论模型：Agent Learning 的递归同构

我们可以尝试将 Agent 的学习过程形式化为一种**递归同构（Recursive Isomorphism）**结构。即：**“学习”本身也是一个 Agent 解决问题的过程**。

在这个框架下，优化者（Optimizer）和执行者（Executor）遵循完全相同的运作模式：**Observation $\to$ Action** 循环，区别仅在于它们处理的“数据类型”不同。

我们可以定义一个通用的 Agent 策略函数 $\pi$，在第 $k$ 层级：

$$
A_k = \pi_k(O_k; \theta_k)
$$

这个结构可以像俄罗斯套娃一样无限展开：

1.  **Level 0 (Execution Layer)**：
    -   **目标**：解决具体任务（如写代码、订票）。
    -   **Observation ($O_0$)**：具体的任务环境、用户指令、API 返回值。
    -   **Action ($A_0$)**：生成文本、调用工具。
    -   **参数 ($\theta_0$)**：Prompt、Context、Tools。

2.  **Level 1 (Optimization Layer)**：
    -   **目标**：优化 Level 0 Agent 的表现。
    -   **Observation ($O_1$)**：Level 0 Agent 的**跑测结果**（Trajectories）、错误日志、评测分数。
    -   **Action ($A_1$)**：**修改 $\theta_0$**（优化 Prompt、调整工具集、注入新案例）并**启动新的实验**。
    -   **参数 ($\theta_1$)**：优化策略（Meta-Prompt）、反思逻辑。

3.  **Level N (Meta-Optimization Layer)**：
    -   **目标**：优化 Level $N-1$ Agent 的学习效率。
    -   **Observation ($O_N$)**：Level $N-1$ 的优化历史和改进曲线。
    -   **Action ($A_N$)**：**修改 $\theta_{N-1}$**（例如调整“如何修改Prompt”的策略）。

我们可以用伪形式化的方式来表达这种递归关系（注意：这更多是一种概念性的类比，而非严格的数学定义）：

$$
\begin{cases}
O_{k+1} \leftarrow \text{History}(\pi_k) & \text{// 高层的观测是低层的历史} \\
A_{k+1} \to \text{Update}(\theta_k) & \text{// 高层的动作是更新低层的参数}
\end{cases}
$$

其中 $\text{History}(\cdot)$ 表示对低层 Agent 执行轨迹的某种聚合或摘要，$\text{Update}(\cdot)$ 表示对低层参数的修改操作。这种视角的价值在于统一了 **"做任务"** 和 **"学任务"** 的范式。虽然在当前的算力约束下，高阶项（$k \geq 2$）的优化显得极其昂贵且收益递减。

### 抽象视角：Agent 的本质是万能翻译器

Agent 的本质可以被抽象为一种不同模态之间的**通用翻译器**。

尽管 AI 技术看似眼花缭乱，但我们面对的实体其实是有限的：**Text（文字）、Audio（音频）、Image（图像）、Code（代码）、Video（视频）、File（文件）** ...

很多让我们眼前一亮的 AI 产品，都是这些实体转换的重新排列组合：
- **ChatGPT**：Text → Text
- **Cursor**：Text → Code
- **Nano-Banana**：Text → Image
- **Veo3**：Text → Video
- **DeepWiki**：Code → Text
- **Manus**: Text → File

从过程上看，绝大多数 Agent 产品遵循一个共同的范式：**Language → Action (Code)**。语言（用户的意图）被翻译成代码，而代码被执行后产生了与物理/数字世界的真实交互。

**把这些“语言”的转换互相组合，在一些微妙的条件下，可能就会诞生全新的产品形态。**

### 集群视角：Multi-Agent 的递归、激活与协同税

虽然初始架构应保持简洁，但 Multi-Agent System (MAS) 仍有其价值。关键在于理解它**如何起效以及何时起效**，其核心在于：**递归（Recursion）**与**激活（Activation）**。

1.  **递归（Recursion）**：这是对"Budget"的极致扩展。如果一个 Main Agent 的能力上限是 30 步（Step），那么通过调用一个 Sub-Agent（同样具备 30 步能力），我们理论上可以将任务执行深度扩展到 $30 \times 30 = 900$ 步。三层的结构甚至可以继续递归为 $30 \times 30 \times 30$。目前之所以没有大规模应用更深层的递归，并非理论不可行，而是受到多重现实约束：模型的 Context 和稳定性不足以支撑如此长程的因果链条；大量外部数据源和工具生态尚未被有效打通；长链路执行带来的延迟和成本在商业上难以接受；以及调试和可解释性的困难使得生产环境部署风险过高。
2.  **激活（Activation）**：这是对“专业度”的增强。通过为 Sub-Agent 注入特定的 System Prompt、Context 和 Action Space（工具集），或者加载不同的微调权重，我们实际上是在“激活”一个特定领域的专家。

然而，MAS 也引入了不可忽视的**协同税（Coordination Tax）**。

协同税是指 Agent 之间在传递信息、对齐意图和等待响应时所消耗的成本（Token、时间、精度）。

- **层级式结构（Hierarchical）**：如果我们采用上下级的树状结构，每个 Agent 只与其直接上下级通信，单次任务的沟通次数与树的深度相关，整体复杂度可控。
- **网状结构（Mesh/All-to-All）**：如果允许 $n$ 个 Agent 之间两两自由对话，潜在的沟通通道数量将增长为 $O(n^2)$，协调成本急剧上升。

过高的协同税往往会让系统陷入混乱和低效，因此，**在架构设计中权衡"递归收益"与"协同税"，是设计高效 MAS 的关键。**

## 二、关于产品的思考

### 全栈敏感度

设计一款优秀的 AI 产品，需要同时具备两种敏感度：
- **对人类敏感**：设计好的交互，理解人性的弱点与需求。
- **对机器敏感**：设计好的 AI，理解模型的边界与能力的本质。

只有对它们都敏感，才能在技术与人性之间找到完美的平衡点。

### 产品定义即人群筛选

在 AI 时代，产品定义的作用被放大了：**它本质上是在进行人群的“预筛选”**。

如果你定义一个 Coding Agent，你的使用者大概率对 AI 有一定认知，对代码逻辑也不完全是小白。这种用户带来的数据是结构化的、反馈是精准的。但如果你定义一个类似 General Agent 的泛用产品，你面对的将是千人千面的用户画像和参差不齐的知识水平。

**你把产品定义成什么样，就会得到什么样的用户，进而得到什么样的数据和反馈。** 这种数据飞轮的初始方向，往往在产品定义的瞬间就已经决定了。对于希望通过用户数据迭代模型的团队来说，这一点至关重要。

### Agent 产品与开放世界游戏设计

如果有人问我 Agent 产品该如何设计，我会推荐他去研究 **《荒野大镖客2》** 和 **《星际拓荒》**。

这两款游戏完美展示了 **开放世界（Agentic Exploration）** 和 **剧情向内容（Workflow/SOP）** 是如何有机结合的。
- 在《荒野大镖客2》中，主线剧情提供了强有力的引导（Workflow），但玩家在任务之外拥有极高的自由度，可以与环境交互、触发随机事件（Agentic）。
- 《星际拓荒》则更进一步，知识本身就是引导，玩家通过探索获得的信息自然地驱动下一步的行动规划。

做 Agent 产品和做开放世界游戏有着惊人的相似之处。这里的"开放世界"是双关的：对 Agent 而言，它需要在给定的 Action Space 内拥有足够的探索自由，而非被死板的 SOP 禁锢；对用户而言，使用产品的体验本身也应如同开放世界游戏——有主线引导方向，也有自由探索的余地。Workflow 保证下限，Agent 的适应性创造惊喜，用户的自主权则决定体验的上限。

### 从 MVC 到 Agent-Centric 架构

传统的 MVC 架构正在被重构。

- **Controller -> Agent Core**：过去被动、硬编码的控制器，正在被一个能自主规划和调用工具的智能“大脑”所取代。
- **Model -> Capabilities**：模型层不再仅仅是数据存储，而是演变成了提供能力和数据的工具集（Tools）。
- **View -> Generative Interface**：视图层正在经历最激进的变革。它不再是静态的 UI 渲染，而是演变成了 **Generative Interface（生成式交互界面）**，界面是流动的，是根据 Agent 的意图和用户的即时需求动态生成的，各个团队也在这种交互上进行不同的尝试。

### 两种“产品经理”的博弈

在 AI 领域，我观察到一种有趣的双重产品设计视角：
- **产品经理**在为**用户**设计产品（UI/UX、功能、场景）。
- **算法/Researcher** 在为**模型**设计产品。

这两个“产品”实际上是在互相制约和对齐的。优秀的产品体验需要模型能力的支撑，而模型能力的释放又受限于产品交互的形态。

如果产品经理不懂模型边界，设计出的功能就是空中楼阁；如果算法不懂用户场景，优化出的模型/agent就是闭门造车。

### 判断任务是否适合 AI

1.  **Hard for Humans（对人类有负担）**：这里的"难"不仅指智力难度，也包括**繁琐、重复、耗时**。很多成功的 AI 应用（如邮件分类、语音转文字、批量图片处理）恰恰是做"人类觉得简单但不愿意做"的事——简单但量大、重复且枯燥。这类任务同样是 AI 的价值高地，因为它解放了人类的时间和注意力。
2.  **Digital（数字化程度高）**：任务的输入和输出必须是数字化的，或者容易被数字化的。这是 AI 介入的物理前提。
3.  **Data Easy to Get or Create（数据易获取）**：这是 AI 学习的养料。如果没有数据，再强的模型也无米下锅。

在评估一个新的 AI 想法时，我会反复用这把尺子去衡量：我们是不是在做一个"人类既不觉得难、也不觉得烦"、"物理世界重度依赖"或者"数据极度匮乏"的产品？如果是，那多半是死路一条。

### AI 产品的行为设计公式：B = MAP

借用 BJ Fogg 的行为模型（Fogg Behavior Model），我认为 AI 产品设计也遵循 $B = MAP$ 的公式：
**Behavior（行为）= Motivation（动机）× Ability（能力）× Prompt（提示）**

1.  **Motivation（用户动机）**：用户为什么要在这一刻使用你的 AI？是为了解决急迫的 Bug（高动机），还是只是闲得无聊想聊两句（低动机）？AI 产品往往容易陷入“炫技”的陷阱，而忽略了用户最朴素的动机——省力、省时或获得情感慰藉。
2.  **Ability（用户能力）**：这里对应的是**易用性（Simplicity）**。目前的 AI 产品往往对用户能力要求过高（比如需要写复杂的 Prompt）。优秀的产品应该**赋予用户能力**，而不是**消耗用户能力**。如果使用你的产品需要用户先学习一本“提示词工程指南”，那 Ability 就太低了。理想的情况是，把操作门槛降到接近零，让用户感觉自己变强了。
3.  **Prompt（产品提示）**：产品在恰当的时机（Context）给出的引导、按钮或通知，提醒用户“现在可以用 AI 来解决这个问题”。

AI 产品经理的任务，就是在动机（M）足够时，最大化用户的能力（A），并提供精准的提示（P）来促成行为的发生。

## 三、关于行业的思考

### 真正的壁垒：元能力

在这个时代，AI 产品高度依赖基础模型。而基础模型正在成为一种公共基础设施（Utility）——就像电力和互联网一样，人人都能接入，人人都能使用。

当所有团队都能调用同一水平的底层智能时，一个功能点的领先可能只需要几周就会被追平。**壁垒便不再是产品目前的某个功能点，而是"制造这个能力的元能力"**。

这里的"元能力"是指，团队自己构建的 Agent 能否反过来用于团队内部的迭代流程？例如，用自家的 Agent 来加速自身产品的开发，分析竞品和论文。当 Agent 成为协助自身产品演进的工具时，便形成了一个自我增强的正反馈循环——用 AI 来做 AI。当然同时也指，团队成员本身的沟通迭代速度。

对于早期的 AI 创业团队而言，**速度就是壁垒**。在巨头尚未反应过来的窗口期内，谁能更快地将 Insight 转化为 Product Feature，谁能更快地通过用户反馈完成数据闭环，谁就能在夹缝中抢先建立起用户心智和数据护城河。

### 迭代文明，而非个体

当我们评估一个 LLM 时，往往习惯于关注个体表现：它这次回答得准不准？下次这个 Case 有没有解决？这种视角是把 AI 当作“一个人”来看待。

但如果我们把视野拉高，一个成功的 AI 产品拥有海量用户，成千上万个 Agent 实例在平行时空里与用户交互、学习、试错。这时候，我们面对的不再是一个单独的智能体，而是一个正在演化的**数字文明**。

**迭代一个 AI 系统，关注整个群体的进化趋势。** 

个体往往追求在特定任务上效果的极致（Overfitting），而群体进化的关键在于**泛化（Generalization）与迁移（Transfer）**。我们需要思考的是：
- 某个 Agent 在 A 任务中学到的经验，如何被抽象并迁移到完全不同的 B 任务中？
- 这个系统内的知识如何像文明传承一样被积累和共享，而不是随着会话结束而消散？
- 如何设计机制，让这个“文明”在海量交互中自动涌现出更高级的智能，而不是依赖工程师手动修补每一个 Bug？

### 创业者的夹缝生存：寻找计算的新平衡

AI 创业团队正处于一个尴尬的“夹缝”之中：
- **往模型层（Model Layer）靠**：越往底层走，你面临的是与科技巨头（Google, OpenAI, Meta）在算力、数据和人才上的正面绞杀。这是一个资本密集型的游戏。
- **往工作流层（Workflow Layer）靠**：越往上层走，你变得越像一个传统的 SaaS。虽然避开了巨头的锋芒，但你又陷入了与成熟软件厂商的存量竞争，且 AI 的附加值容易被稀释。

要在夹缝中生存，不仅需要找到独特的生态位，更关键的是要找到**Offline（离线）与 Online（在线）计算量的新平衡**。

我们习惯于把 Offline Compute 等同于 Training，把 Online Compute 等同于 Inference。但对于 Agent 产品来说，这种划分太狭隘了。

**把 Online 的计算压力分摊到 Offline 去**，利用大量的离线算力来换取极致的在线体验。这种“计算量的时间套利”，可能是创业团队构建差异化体验的一条捷径。

## 四、关于人的思考

### Fact vs. Truth：偏好的困境

黄仁勋年中问了 Sam Altman 一个深刻的问题：**"Fact is what is. Truth is what it means."**

AI 可以轻易地学习和记忆海量的 Facts（事实），但 Truth（真相/真理）往往是主观的，它取决于视角、文化、价值观和背景。

大部分的开放性问题并没有绝对的 Reward。Instagram 用户的偏好和 X (Twitter) 用户的偏好截然不同；甚至同一个用户，上个月的偏好和下个月的偏好也会发生漂移。

这意味着**构建一个绝对通用的 Reward Model 几乎是不可能的**。具体价值观的对齐，本质上是对目标用户群体偏好的捕捉和适应。

### 奖励的梯度与内心的宗教

人追求的往往不是 Reward 的绝对值（比如 100 块钱），而是 **Reward 的梯度**（从没钱到有钱的增长感）。正是这种“局势正在变好”的希望感（Hope），即 **Reward Function 的n阶导数为正**，驱动着人类不断前行。

我们可以用一个**直觉性的类比**（而非严格的数学模型）来描述这种"希望感"：

$$
\text{Hope}(t) \sim w_1 \frac{dR}{dt} + w_2 \frac{d^2R}{dt^2} + \cdots
$$

其中 $R(t)$ 是当前的 Reward 状态，$w_n$ 是递减的权重系数（确保高阶项的影响逐渐衰减）。这个表达式意在说明：我们真正迷恋的，往往不是 $R(t)$ 本身，而是它的变化趋势——一阶导数（增速）、二阶导数（加速度）等正向信号的叠加。

然而，物理世界的增长总是有极限的。当增长放缓甚至停滞（梯度归零），人就会感到痛苦和迷茫。

这时，人类往往通过两种方式自救：
1.  **重构奖励函数（Rewiring Reward Function）**：主动修正优化的目标变量（Objective Variable）。不再单纯追求外部物理世界中边际收益递减的指标（如金钱），而是将价值锚点迁移至精神或道德维度。例如斯多葛学派（Stoicism），通过认知重塑（Cognitive Reframing）将外部不可控的负向反馈（痛苦）重新编码为正向的价值信号（磨练），从而实现了 Reward 机制的自洽与鲁棒性。
2.  **向内探索（Latent Space Exploration）**：不再依赖物理世界的稀疏反馈，而是转向内部，在精神世界的 Latent Space 中寻找满足。通过在某种高维特征空间中的漫游与重组，构建出源源不断的内在奖励（Intrinsic Reward）。

这或许从计算视角解释了宗教和哲学的演化必然性：**它们构建了一个具有 Infinite Horizon（无限视界）的优化目标，并在 Latent Space 中建立了一套能够持续产生 Dense Intrinsic Reward（稠密内在奖励）的反馈机制。**

# 结语

2025 年上半年，无论是 LLM 在 RL 方向的进展，还是 Agent 产品的集中爆发，都把行业情绪推到了高点。到了下半年，新鲜的产品定义并不多，Research 也更多是在上半年的思路上做延展与工程化。2026会怎么样呢？我很期待，我很担忧。